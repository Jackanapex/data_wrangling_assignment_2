{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Email Data Transformation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from email.utils import parsedate_to_datetime\n",
    "# below is only needed if visualization is needed\n",
    "import math\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import percentileofscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the structured data from the profiling stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the input and output directories and files\n",
    "INPUT = 'input'\n",
    "MAILDIR = 'maildir'\n",
    "OUTPUT = 'output'\n",
    "STRUCTURED_DATA = 'structured_data_'\n",
    "PROFILING_OUTPUT = 'profiling_output_'\n",
    "TRANSFORMATION1_OUTPUT = 'analysis1_transformation_and_enriching_output_'\n",
    "TRANSFORMATION2_OUTPUT = 'analysis2_transformation_and_enriching_output_'\n",
    "ANALYSIS1_VIS_OUTPUT = 'analysis_1_visualisation_output_'\n",
    "ANALYSIS2_VIS_OUTPUT = 'analysis_2_visualisation_output_'\n",
    "IMPORT_SUCCESS = 'SUCCESS'\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    INPUT,\n",
    "    MAILDIR\n",
    ")\n",
    "output_dir = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT\n",
    ")\n",
    "structured_data_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    STRUCTURED_DATA + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "profiling_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    PROFILING_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.csv'\n",
    ")\n",
    "transformation1_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    TRANSFORMATION1_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.csv'\n",
    ")\n",
    "transformation2_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    TRANSFORMATION2_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.csv'\n",
    ")\n",
    "analysis_question_1_visualisation_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    ANALYSIS1_VIS_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.html'\n",
    ")\n",
    "analysis_question_2_visualisation_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    ANALYSIS2_VIS_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.html'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is needed if we'd like to read the df from saved\n",
    "# structured email data files in csv format\n",
    "# Please note: the target location is in the output folder in the project !\n",
    "STRUCTURED_DATA_FN_PATTERN = '^structured_data_' # provide the saved structured data csv file name regex pattern\n",
    "list_of_df = []\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    if files:\n",
    "        for file in files: # iterate through all files matching the pattern\n",
    "            if re.match(STRUCTURED_DATA_FN_PATTERN, file) is not None:\n",
    "                this_df = pd.read_csv(os.path.join(root,file), index_col=None, header=0)\n",
    "                list_of_df.append(this_df) # and append each one into the whole list for combination\n",
    "df = pd.concat(list_of_df, axis=0, ignore_index=True) # combine and form the full email data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse the data and apply curations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_and_sort_comma_separated_email_addresses(s):\n",
    "    \"\"\"\n",
    "    This is a helper function to tidy up comma delimited strings\n",
    "    by removing all whitespace, newlines and carriage returns\n",
    "    and then sorting the strings alphabetically\n",
    "\n",
    "    parameters:\n",
    "        s: the string to tidy up\n",
    "    returns:\n",
    "        the tidied up string\n",
    "    \"\"\"\n",
    "    if type(s) == str:\n",
    "        return ','.join(sorted([re.sub(r'[\\r\\n\\s]+','',i) for i in s.split(',')]))\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the comma delimited email strings in the From, To, Cc and Bcc fields into lists\n",
    "# so they can be cleansed iteratively. After the treatment, convert them back to comma-delimited strings\n",
    "# so that they can be properly de-duplicated.\n",
    "df['From'] = df['From'].apply(lambda x: _clean_and_sort_comma_separated_email_addresses(x))\n",
    "df['To'] = df['To'].apply(lambda x: _clean_and_sort_comma_separated_email_addresses(x))\n",
    "df['Cc'] = df['Cc'].apply(lambda x: _clean_and_sort_comma_separated_email_addresses(x))\n",
    "df['Bcc'] = df['Bcc'].apply(lambda x: _clean_and_sort_comma_separated_email_addresses(x))\n",
    "# Also parse the Date values into UTC datetime stamps so they are comparable and can be properly deduplicated\n",
    "df['Date'] = df['Date'].apply(lambda x: parsedate_to_datetime(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curation 1 Extract the relevant fields from the df\n",
    "df_extracted = df[['Message-ID', 'From', 'To', 'Cc', 'Bcc', 'Date', 'Subject']].copy()\n",
    "# Curation 2 Unify the data to lower cases so they can be properly deduplicated\n",
    "for column in df_extracted.columns:\n",
    "    # this is only needed for from, to, cc and bcc columns\n",
    "    # we don't make the Subject all lower case because the upper/lower cases\n",
    "    # carry actual information, and we don't want to lose that\n",
    "    if column in ['From', 'To', 'Cc', 'Bcc']:\n",
    "        df_extracted[column] = df_extracted[column].str.lower() \n",
    "# Curation 3 Convert the Date column from datetime.datetime to np.datetime64 so it can be properly deduplicated\n",
    "df_extracted['Date'] = pd.to_datetime(df_extracted['Date'], utc=True)\n",
    "# Curation 4 Remove invalid dates\n",
    "# Invalid dates are defined as all dates before 1998-01-01 and after 2002-12-31 (exclusive)\n",
    "df_extracted = df_extracted.loc[\n",
    "    ((df_extracted['Date'].dt.year*100 + df_extracted['Date'].dt.month) >= 199801) & \n",
    "    ((df_extracted['Date'].dt.year*100 + df_extracted['Date'].dt.month) <= 200212)\n",
    "]\n",
    "# Curation 5 Remove the duplicates (the same email can be saved as multiple copies in different people's mailboxes)\n",
    "df_extracted = df_extracted.groupby(['From', 'To', 'Cc', 'Bcc', 'Date', 'Subject'], dropna=False)['Message-ID'].last().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curation 6 We are only interested in genuine human email addresses\n",
    "# so the valid email addresses are defined as matching the following regex\n",
    "# ^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$\n",
    "# By doing this we can filter out system email addresses\n",
    "# From column\n",
    "email_regex_pattern = r\"^[a-z0-9.'_%+-<]+@[a-z0-9.-]+\\.[a-z]{2,}>*?$\"\n",
    "df_extracted.loc[~df_extracted['From'].str.match(email_regex_pattern),['From']] = np.nan\n",
    "# To, Cc and Bcc columns:\n",
    "# convert string to lists first\n",
    "df_extracted['To'] = df_extracted['To'].apply(lambda x: x.split(',') if type(x) == str else x)\n",
    "df_extracted['Cc'] = df_extracted['Cc'].apply(lambda x: x.split(',') if type(x) == str else x)\n",
    "df_extracted['Bcc'] = df_extracted['Bcc'].apply(lambda x: x.split(',') if type(x) == str else x)\n",
    "# Then iterate through each list and keep only the valid email addresses\n",
    "email_regex = re.compile(email_regex_pattern)\n",
    "df_extracted['To'] = df_extracted['To'].apply(lambda x: [i for i in x if email_regex.match(i)] if isinstance(x, list) else x)\n",
    "df_extracted['Cc'] = df_extracted['Cc'].apply(lambda x: [i for i in x if email_regex.match(i)] if isinstance(x, list) else x)\n",
    "df_extracted['Bcc'] = df_extracted['Bcc'].apply(lambda x: [i for i in x if email_regex.match(i)] if isinstance(x, list) else x)\n",
    "# If a list is reduced to zero after the cleansing, change the value to np.nan\n",
    "df_extracted['To'] = df_extracted['To'].apply(lambda x: np.nan if (isinstance(x, list) and not x) else x)\n",
    "df_extracted['Cc'] = df_extracted['Cc'].apply(lambda x: np.nan if (isinstance(x, list) and not x) else x)\n",
    "df_extracted['Bcc'] = df_extracted['Bcc'].apply(lambda x: np.nan if (isinstance(x, list) and not x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curation 7 Remove records with empty From, empty To+Cc+Bcc and empty Date fields, because they are not useful for our analysis\n",
    "df_extracted = df_extracted[\n",
    "    ~(\n",
    "        df_extracted['From'].isna() | \n",
    "        (df_extracted['To'].isna() & df_extracted['Cc'].isna() & df_extracted['Bcc'].isna()) | \n",
    "        df_extracted['Date'].isna()\n",
    "    )\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and normalise the data into separate tables\n",
    "We need the following 3 tables to perform the analysis:\n",
    "- email_from: a table containing the relationship between Message-ID and From\n",
    "- email_to: a table containing the relationship between Message-ID, and the email address in To, Cc and Bcc, and the type of messaging (to, cc or bcc)\n",
    "- email_datetime: a table containing the relationship between Message-ID and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the email_from table.\n",
    "df_email_from = df_extracted[['Message-ID', 'From']].copy()\n",
    "# get the email_table\n",
    "df_email_to = df_extracted[['Message-ID', 'To', 'Cc', 'Bcc']].copy()\n",
    "# get the email_datetime table\n",
    "df_email_datetime = df_extracted[['Message-ID', 'Date']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the email_to table we need to normalise it so that each email_address contains only one value\n",
    "df_email_to = df_email_to.melt(id_vars=['Message-ID'], value_vars=['To', 'Cc', 'Bcc'])\n",
    "df_email_to = df_email_to.rename(columns={'variable': 'message_type', 'value': 'message_to'})\n",
    "df_email_to = df_email_to.explode('message_to').reset_index(drop=True)\n",
    "df_email_to.dropna(inplace = True, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for analysis question 1 : the change of the external communication frequencies before and after \n",
    "# the Enron scandal, we need to join the email_from and email_to tables to get the information of\n",
    "# whether the email is internal or external. \n",
    "# In the enriching step, we then need to join the product table to the email_datetime\n",
    "# table, and aggregate the total number of emails grouped by both month and internal/external.\n",
    "df_analysis_1 = df_email_from.merge(df_email_to, on='Message-ID', how='inner')\n",
    "df_analysis_1 = df_analysis_1.merge(df_email_datetime, on='Message-ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for analysis question 2 : the Enron internal network, we need to join the email_from and email_to \n",
    "# tables to get the information of sender-receiver pairs. We then need to order the sender and receiver\n",
    "# pairs alphabetically so that the pairs are no longer vectorized.\n",
    "# In the enriching step, we then need to aggregate the number of total emails grouped by the pairs.\n",
    "# The number of total emails can be a presentation of the strength of the relationship between the pairs.\n",
    "df_analysis_2 = df_email_from.merge(df_email_to, on='Message-ID', how='inner')\n",
    "# remove the email pairs with the same sender and receiver\n",
    "df_analysis_2 = df_analysis_2[df_analysis_2['From'] != df_analysis_2['message_to']].copy().reset_index(drop=True)\n",
    "df_analysis_2['email_A'] = np.minimum(df_analysis_2['From'], df_analysis_2['message_to']) \n",
    "df_analysis_2['email_B'] = np.maximum(df_analysis_2['From'], df_analysis_2['message_to']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich the transformed data for the analysis questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis question 1, we need to create the following columns:\n",
    "1. communication_type: internal or external\n",
    "2. communication_month: a higher level of time granularity (year-month)\n",
    "\n",
    "please note that one email can have both internal and external from-and-to address pairs,\n",
    "and we treat all email address pairs in the same email as separate communication instances\n",
    "The results are visualised in the bar chart below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate if a pair of communication emails is of the relationship internal or external by their domains\n",
    "internal_email_pattern = r\"^[a-z0-9.'_%+-<]+@enron\\.com[>]?$\"\n",
    "df_analysis_1['communication_type'] = 'external'\n",
    "df_analysis_1.loc[\n",
    "    df_analysis_1['From'].str.contains(internal_email_pattern) &\n",
    "    df_analysis_1['message_to'].str.contains(internal_email_pattern),\n",
    "    'communication_type'\n",
    "] = 'internal'\n",
    "df_analysis_1['communication_month'] = df_analysis_1['Date'].dt.strftime('%Y-%m')\n",
    "# Aggregate the result to the year-month level\n",
    "df_analysis_1_result = df_analysis_1.groupby(['communication_type', 'communication_month'], dropna=False).size().reset_index().rename(columns={0:'instance_count'})\n",
    "df_analysis_1_result.to_csv(transformation1_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the line chart\n",
    "fig = px.line(df_analysis_1_result, x='communication_month', y='instance_count', color='communication_type', title='Enron Monthly Email Communication Intances by Internal/External')\n",
    "\n",
    "# Save the output for further analysis (see the attached summary report)\n",
    "fig.write_html(analysis_question_1_visualisation_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis question 2, we need to create the following caculated column:\n",
    "1. communication_weight\n",
    "\n",
    "We'd like to define the communication_weight of each instance per below:\n",
    "- for instances sending directly To an email address, the weight is 1\n",
    "- for instances sending as Cc to an email address, the weight is 0.2\n",
    "- for instances sending as Bcc to an email address, the weight is 0.5\n",
    "\n",
    "similar to analysis question 1, multiple pairs in the same email are considered to contribute\n",
    "to a pair of email addresses as separate communication instance the results are visualised in a network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weight of communication by its type (To, Cc or Bcc)\n",
    "df_analysis_2['communication_weight'] = 1\n",
    "df_analysis_2.loc[df_analysis_2['message_type'] == 'Cc', 'communication_weight'] = 0.2\n",
    "df_analysis_2.loc[df_analysis_2['message_type'] == 'Bcc', 'communication_weight'] = 0.5\n",
    "# sum up the weight and group by the email pairs\n",
    "df_analysis_2_result = df_analysis_2.groupby(['email_A', 'email_B'], dropna=False)['communication_weight'].sum().reset_index()\n",
    "df_analysis_2_result.to_csv(transformation2_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we need to keep the number of nodes viable for a network graph \n",
    "so we are only interested in Enron internal significant connections which\n",
    "have the communciation weights over 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the effective internal email pairs\n",
    "internal_email_pattern = r\"^[a-z0-9.'_%+-<]+@enron\\.com[>]?$\"\n",
    "df_analysis_2_result_effective = df_analysis_2_result[\n",
    "    df_analysis_2_result['email_A'].str.contains(internal_email_pattern) &\n",
    "    df_analysis_2_result['email_B'].str.contains(internal_email_pattern) &\n",
    "    (df_analysis_2_result['communication_weight'] >= 100.0)\n",
    "].copy().reset_index(drop=True)\n",
    "\n",
    "# Instantiate the network from the selected email pairs and communication weights\n",
    "G = nx.from_pandas_edgelist(df_analysis_2_result_effective, 'email_A', 'email_B', ['communication_weight'])\n",
    "pos = nx.spring_layout(G, k=2/math.sqrt(len(G.nodes())))  # you can use other layout algorithms as well\n",
    "nx.set_node_attributes(G, pos, 'pos')\n",
    "\n",
    "# Calculate sizes based on total communication_weight\n",
    "sizes = {}\n",
    "for node in G.nodes():\n",
    "    sizes[node] = sum(weight for _, _, weight in G.edges(node, data='communication_weight'))\n",
    "nx.set_node_attributes(G, sizes, 'size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the gragh\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "edge_weights = []\n",
    "edge_note = []\n",
    "edge_note_points_x = []\n",
    "edge_note_points_y = []\n",
    "\n",
    "for edge in G.edges(data='communication_weight'):\n",
    "    x0, y0 = G.nodes[edge[0]]['pos'] # extract the locations of node_0 (email) from edges (communication)\n",
    "    x1, y1 = G.nodes[edge[1]]['pos'] # extract the locations of node_1 (email) from edges (communication)\n",
    "    edge_note.append(f\"{edge[0]} - {edge[1]}: {edge[2]}\") # build the edge note list\n",
    "    edge_x.append([x0, x1, None])\n",
    "    edge_y.append([y0, y1, None])\n",
    "    edge_note_points_x.append((x0 + x1)/2) # assuming values positive/get midpoint\n",
    "    edge_note_points_y.append((y0 + y1)/2) # assumes positive vals/get midpoint\n",
    "    edge_weights.append(edge[2])\n",
    "edge_weights_colour_scale = []\n",
    "# use each edge's weight to scale it to a degree of grey\n",
    "for w in edge_weights:\n",
    "    scaled_weight = (1 - percentileofscore(edge_weights, w, 'rank') / 100) * 255\n",
    "    edge_weights_colour_scale.append(f\"rgb({scaled_weight},{scaled_weight},{scaled_weight})\")\n",
    "# Also calculate the minimum and maximum of the degree of grey to be used in the chart color bar\n",
    "colorscale_min = (1 - percentileofscore(edge_weights, min(edge_weights), 'rank') / 100) * 255\n",
    "colorscale_max = (1 - percentileofscore(edge_weights, max(edge_weights), 'rank') / 100) * 255\n",
    "\n",
    "# Build the edge lines, each with a different colour depending on its weight\n",
    "edge_traces = []\n",
    "for _x, _y, _w in zip(edge_x, edge_y, edge_weights_colour_scale):\n",
    "    edge_trace = go.Scatter(\n",
    "        x=_x,\n",
    "        y=_y,\n",
    "        line=dict(color=_w, width=1),\n",
    "        hoverinfo='none',\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    edge_traces.append(edge_trace)\n",
    "\n",
    "# Build the edge hovering points with notes at the medium location of the two nodes\n",
    "edge_note_trace = go.Scatter(\n",
    "    x=edge_note_points_x,\n",
    "    y=edge_note_points_y,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        opacity=0, \n",
    "        color=edge_weights,\n",
    "        colorbar=dict(\n",
    "            title=\"Communication<BR>Weight\"\n",
    "        ),\n",
    "        colorscale=[[0, f\"rgb({colorscale_min},{colorscale_min},{colorscale_min})\"], [1, f\"rgb({colorscale_max},{colorscale_max},{colorscale_max})\"]]\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    hoverinfo='text',\n",
    "    text=edge_note\n",
    ")\n",
    "\n",
    "# Retrieve the nodes locations from the network and plot them in a scatter chart\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = G.nodes[node]['pos']\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=[f\"{node} : {round(G.nodes[node]['size'],2)}\" for node in G.nodes()],\n",
    "    marker=dict(\n",
    "        size=[round(math.sqrt(G.nodes[node]['size'])/2.5,2) for node in G.nodes()]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot all the graph from above on the same planner\n",
    "fig = go.Figure(\n",
    "    data=[edge_note_trace, node_trace, *edge_traces],\n",
    "    layout=go.Layout(\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        title=\"Enron Major Connection Network\",\n",
    "        margin=dict(l=20, r=20, t=25, b=15),\n",
    "        xaxis=dict(\n",
    "            title=None,  # Removes x-axis title\n",
    "            showgrid=False,  # Removes x-axis grid lines\n",
    "            showticklabels=False,  # Removes x-axis tick labels\n",
    "            ticks=\"\"  # Removes x-axis tick marks\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=None,  # Removes x-axis title\n",
    "            showgrid=False,  # Removes x-axis grid lines\n",
    "            showticklabels=False,  # Removes x-axis tick labels\n",
    "            ticks=\"\"  # Removes x-axis tick marks\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the output for further analysis (see the attached summary report)\n",
    "fig.write_html(analysis_question_2_visualisation_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
