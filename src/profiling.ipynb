{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Email Data Loading and Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "from email.utils import parsedate_to_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the helper function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_email_metadata(filename):\n",
    "    \"\"\"\n",
    "    this is a helper function which extracts the metadata from an email file with Regex\n",
    "    parameters:\n",
    "        filename: the filename of the email file\n",
    "    returns:\n",
    "        meta_output: a dictionary containing the metadata\n",
    "    \"\"\"\n",
    "    meta_output = {\n",
    "        'Message-ID':None,\n",
    "        'Date':None,\n",
    "        'From':None,\n",
    "        'To':None,\n",
    "        'Cc':None,\n",
    "        'Bcc':None,\n",
    "        'Subject':None,\n",
    "        'Mime-Version':None,\n",
    "        'Content-Type':None,\n",
    "        'Content-Transfer-Encoding':None,\n",
    "        'X-From':None,\n",
    "        'X-To':None,\n",
    "        'X-cc':None,\n",
    "        'X-bcc':None,\n",
    "        'X-Folder':None,\n",
    "        'X-Origin':None,\n",
    "        'X-FileName':None\n",
    "    }\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "    # regex for Message-ID\n",
    "    _message_id = re.search(r\"^Message-ID:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for Date\n",
    "    _date =  re.search(r\"^Date:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for From\n",
    "    _from = re.search(r\"^From:[^\\S\\r\\n]+(.*?)(?=[\\r\\n]^To|[\\r\\n]^Subject|[\\r\\n]^Cc|[\\r\\n]^Mime-Version)\", content, flags=re.MULTILINE | re.DOTALL)\n",
    "    # regex for To\n",
    "    _to = re.search(r\"^From:[^\\S\\r\\n]+\\S*?[\\r\\n]{1}^To:[^\\S\\r\\n]+(.*?)(?=[\\r\\n]^Subject|[\\r\\n]^Cc|[\\r\\n]^Mime-Version)\", content, flags=re.MULTILINE | re.DOTALL)\n",
    "    # regex for Subject\n",
    "    _subject = re.search(r\"^Subject:[^\\S\\r\\n]+(.*?)(?=[\\r\\n]^Cc|[\\r\\n]^Mime-Version)\", content, flags=re.MULTILINE | re.DOTALL)\n",
    "    # regex for Cc\n",
    "    _cc = re.search(r\"^Cc:[^\\S\\r\\n]+(.*?)(?=[\\r\\n]^Mime-Version)\", content, flags=re.MULTILINE | re.DOTALL)\n",
    "    # regex for Mime-Version\n",
    "    _mime_version = re.search(r\"^Mime-Version:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for Content-Type\n",
    "    _content_type = re.search(r\"^Content-Type:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for Content-Transfer-Encoding\n",
    "    _content_transfer_encoding = re.search(r\"^Content-Transfer-Encoding:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for Bcc\n",
    "    _bcc = re.search(r\"^Bcc:[^\\S\\r\\n]+(.*?)(?=[\\r\\n]^X-From)\", content, flags=re.MULTILINE | re.DOTALL)\n",
    "    # regex for X-From\n",
    "    _x_from = re.search(r\"^X-From:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-To\n",
    "    _x_to = re.search(r\"^X-To:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-cc\n",
    "    _x_cc = re.search(r\"^X-cc:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-bcc\n",
    "    _x_bcc = re.search(r\"^X-bcc:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-Folder\n",
    "    _x_folder = re.search(r\"^X-Folder:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-Origin\n",
    "    _x_origin = re.search(r\"^X-Origin:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    # regex for X-FileName\n",
    "    _x_filename = re.search(r\"^X-FileName:[^\\S\\r\\n]+(.*)$\", content, flags=re.MULTILINE)\n",
    "    \n",
    "    meta_output['Message-ID'] = _message_id.group(1) if _message_id else None\n",
    "    meta_output['Date'] =  _date.group(1) if _date else None\n",
    "    meta_output['From'] = _from.group(1) if _from else None\n",
    "    meta_output['To'] = _to.group(1) if _to else None\n",
    "    meta_output['Cc'] = _cc.group(1) if _cc else None\n",
    "    meta_output['Bcc'] = _bcc.group(1) if _bcc else None\n",
    "    meta_output['Subject'] = _subject.group(1) if _subject else None\n",
    "    meta_output['Mime-Version'] = _mime_version.group(1) if _mime_version else None\n",
    "    meta_output['Content-Type'] = _content_type.group(1) if _content_type else None\n",
    "    meta_output['Content-Transfer-Encoding'] = _content_transfer_encoding.group(1) if _content_transfer_encoding else None\n",
    "    meta_output['X-From'] = _x_from.group(1) if _x_from else None\n",
    "    meta_output['X-To'] = _x_to.group(1) if _x_to else None\n",
    "    meta_output['X-cc'] = _x_cc.group(1) if _x_cc else None\n",
    "    meta_output['X-bcc'] = _x_bcc.group(1) if _x_bcc else None\n",
    "    meta_output['X-Folder'] = _x_folder.group(1) if _x_folder else None\n",
    "    meta_output['X-Origin'] = _x_origin.group(1) if _x_origin else None\n",
    "    meta_output['X-FileName'] = _x_filename.group(1) if _x_filename else None\n",
    "\n",
    "    return meta_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_path_after_target(path, target=\"maildir\"):\n",
    "    \"\"\"\n",
    "    This is a helper function to extract the path one level after \"maildir\"\n",
    "    for example if the input is \"maildir/lay-k/somefolder\" the output will be \"somefolder\"\n",
    "    if the input is \"maildir/lay-k\" the output will be np.nan\n",
    "\n",
    "    parameters:\n",
    "        path: the path to extract the folder name from\n",
    "        target: the target folder name to extract the path one level after it\n",
    "    returns:\n",
    "        the path one level after the target folder name\n",
    "    \"\"\"\n",
    "    # Split the path into its individual components\n",
    "    parts = path.split(os.sep)\n",
    "\n",
    "    # Check if the target exists in the parts\n",
    "    if target in parts:\n",
    "        # Get the index of the target\n",
    "        target_index = parts.index(target)\n",
    "\n",
    "        # Return the path one level after target if it exists\n",
    "        if target_index + 2 < len(parts):\n",
    "            return os.path.join(*[parts[i] for i in range(target_index + 2, len(parts))])\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_folder_after_target(path, target=\"maildir\"):\n",
    "    \"\"\"\n",
    "    This is a helper function to extract the folder name immediately after \"maildir\"\n",
    "    for example if the input is \"maildir/lay-k/somefolder\" the output will be \"lay-k\"\n",
    "    if the input is \"maildir\" the output will be np.nan\n",
    "\n",
    "    parameters:\n",
    "        path: the path to extract the folder name from\n",
    "        target: the target folder name to extract the path one level after it\n",
    "    returns:\n",
    "        the folder name immediately after the target folder name\n",
    "    \"\"\"\n",
    "    # Split the path into its individual components\n",
    "    parts = path.split(os.sep)\n",
    "\n",
    "    # Check if the target exists in the parts\n",
    "    if target in parts:\n",
    "        # Get the index of the target\n",
    "        target_index = parts.index(target)\n",
    "\n",
    "        # Return the next folder if it exists\n",
    "        if target_index + 1 < len(parts):\n",
    "            return parts[target_index + 1]\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset_structured_df():\n",
    "    \"\"\"\n",
    "    This is a helper function to reset the structured dataframe\n",
    "    This will be used to iterate through the entire Enron email dataset\n",
    "    and reset the memory after saving to a subset of the data each time\n",
    "    This is necessary because the entire dataset is too large to be loaded from\n",
    "    the source data within a viable length of time, we prefer to load the data and\n",
    "    save the structured copy to the filesystem\n",
    "\n",
    "    parameters:\n",
    "        None\n",
    "    returns:\n",
    "        df_output: a dataframe with the same columns as the structured dataframe\n",
    "    \"\"\"\n",
    "    df_output = pd.DataFrame(columns=[\n",
    "        'maildir_user', \n",
    "        'maildir_folder', \n",
    "        'maildir_file_name', \n",
    "        'maildir_path',\n",
    "        'Message-ID',\n",
    "        'Date',\n",
    "        'From',\n",
    "        'To',\n",
    "        'Cc',\n",
    "        'Bcc',\n",
    "        'Subject',\n",
    "        'Mime-Version',\n",
    "        'Content-Type',\n",
    "        'Content-Transfer-Encoding',\n",
    "        'X-From',\n",
    "        'X-To',\n",
    "        'X-cc',\n",
    "        'X-bcc',\n",
    "        'X-Folder',\n",
    "        'X-Origin',\n",
    "        'X-FileName',\n",
    "        'import_result'\n",
    "    ])\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tidy_up_comma_delimited_strings(s):\n",
    "    \"\"\"\n",
    "    This is a helper function to tidy up comma delimited strings\n",
    "    by removing all whitespace, newlines and carriage returns\n",
    "    and then sorting the strings alphabetically\n",
    "\n",
    "    parameters:\n",
    "        s: the string to tidy up\n",
    "    returns:\n",
    "        the tidied up string\n",
    "    \"\"\"\n",
    "    if type(s) == str:\n",
    "        return ','.join(sorted([re.sub(r'[\\r\\n\\s]+','',i) for i in s.split(',')]))\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the input and output directories and files\n",
    "INPUT = 'input'\n",
    "MAILDIR = 'maildir'\n",
    "OUTPUT = 'output'\n",
    "STRUCTURED_DATA = 'structured_data_'\n",
    "PROFILING_OUTPUT = 'profiling_output_'\n",
    "IMPORT_SUCCESS = 'SUCCESS'\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    INPUT,\n",
    "    MAILDIR\n",
    ")\n",
    "output_dir = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT\n",
    ")\n",
    "structured_data_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    STRUCTURED_DATA + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "profiling_output_path = os.path.join(\n",
    "    os.path.abspath('..'),\n",
    "    OUTPUT,\n",
    "    PROFILING_OUTPUT + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (THIS BLOCK READS ALL FILES IN THE ENRON MAILDIR AND CAN TAKE MORE THAN 30 MINUTES)\n",
    "# iterate through the input_dir and save the first level of directories as user, the second level as folder,\n",
    "# the third level as file_name, and the content of the files as content\n",
    "# the output is the structured Enron email data, saved in the output folder of this project\n",
    "loc_i = 0\n",
    "start_time = datetime.now()\n",
    "processed_count = 0\n",
    "this_user = None\n",
    "df = _reset_structured_df()\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    try:\n",
    "        if files: # only scan the folder if there are files in it\n",
    "            next_user = _get_folder_after_target(root)\n",
    "            folder_tail = _get_path_after_target(root)\n",
    "            if next_user != this_user: # split and save the loaded dataframe to a separate csv file if the user is different\n",
    "                if df.shape[0] > 0:\n",
    "                    df.to_csv(f\"{structured_data_output_path}_{this_user}_{loc_i}.csv\", index=False)\n",
    "                    df = _reset_structured_df() # release the memory of the dataframe\n",
    "                this_user = next_user\n",
    "                loc_i = 0\n",
    "            for file in files: # assign the maildir filesystem information to the structured dataframe\n",
    "                df.loc[loc_i, 'maildir_user'] = next_user\n",
    "                df.loc[loc_i, 'maildir_folder'] = folder_tail\n",
    "                df.loc[loc_i, 'maildir_file_name'] = file\n",
    "                df.loc[loc_i, 'maildir_path'] = os.path.join(root,file)\n",
    "                content = _extract_email_metadata(os.path.join(root,file)) # extract the metadata from each email\n",
    "                for k,v in content.items(): # save the metadata in a strucutred way into the dataframe\n",
    "                    df.loc[loc_i, k] = v\n",
    "                df.loc[loc_i, 'import_result'] = IMPORT_SUCCESS # if everything is successful for a record, mark it as SUCCESS for debugging\n",
    "                loc_i += 1\n",
    "                processed_count += 1\n",
    "                if processed_count % 10000 == 0: # some real-time prompt because the process takes 30+ minutes\n",
    "                    clear_output(wait=True)\n",
    "                    display(f\"{processed_count} files imported, the last 10000 took {datetime.now() - start_time}\")\n",
    "                    start_time = datetime.now()\n",
    "    except Exception as e: # handle the error if any happens - keep track of it and continue the process\n",
    "        df.loc[loc_i, 'import_result'] = f\"ERROR:{e} | ROOT:{root} | FILE:{file}\"\n",
    "        continue\n",
    "if df.shape[0] > 0: # when the process is finished, the final dataframe also needs to be saved\n",
    "    df.to_csv(f\"{structured_data_output_path}_{this_user}_{loc_i}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is needed if we'd like to read the df from saved\n",
    "# structured email data files in csv format\n",
    "# Please note: the target location is in the output folder in the project !\n",
    "STRUCTURED_DATA_FN_PATTERN = '^structured_data_' # provide the saved structured data csv file name regex pattern\n",
    "list_of_df = []\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    if files:\n",
    "        for file in files: # iterate through all files matching the pattern\n",
    "            if re.match(STRUCTURED_DATA_FN_PATTERN, file) is not None:\n",
    "                this_df = pd.read_csv(os.path.join(root,file), index_col=None, header=0)\n",
    "                list_of_df.append(this_df) # and append each one into the whole list for combination\n",
    "df = pd.concat(list_of_df, axis=0, ignore_index=True) # combine and form the full email data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-profiling: Apply simple conversions to the lists of email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the comma delimited emails in the From, To, Cc and Bcc fields into lists\n",
    "# so they can be sorted alphabetically, this will make sure the duplicate values of the To\n",
    "# Cc and Bcc lists can be properly detected regardless of the order of the elements\n",
    "df['From'] = df['From'].apply(lambda x: _tidy_up_comma_delimited_strings(x))\n",
    "df['To'] = df['To'].apply(lambda x: _tidy_up_comma_delimited_strings(x))\n",
    "df['Cc'] = df['Cc'].apply(lambda x: _tidy_up_comma_delimited_strings(x))\n",
    "df['Bcc'] = df['Bcc'].apply(lambda x: _tidy_up_comma_delimited_strings(x))\n",
    "# Also parse the Date values into UTC datetime stamps so they are comparable\n",
    "df['Date'] = df['Date'].apply(lambda x: parsedate_to_datetime(x) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start the profiling with an overview of the dataset column types\n",
    "print(df.dtypes)\n",
    "# and we also would like to know some basic statistics for each column in the dataset\n",
    "print(df.describe(include='all'))\n",
    "# And we want to have a look at if empty/nan values exist in each column\n",
    "print(df.isna().sum())\n",
    "# And we want to have a look at if empty strings exist in each column\n",
    "print(df.applymap(lambda x: x == '').sum())\n",
    "# Check if there are duplicated records in the dataframe\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the overview above, we can deduce that the following columns may be useful for our analysis tasks:\n",
    "# Message-ID, Date, From, To, Cc, Bcc, Subject\n",
    "# However, by looking at the number of unique values in each column, we can see that the Message-ID column\n",
    "# does not identify unique records, so we will need to drop it\n",
    "# Verify the unique-ness of messages - the same email can be determined by the same combination of\n",
    "# From, To, Cc, Bcc, Date and Subject combined together determines duplicated emails stored in different\n",
    "# people's folders\n",
    "df_extracted = df.groupby(['From', 'To', 'Cc', 'Bcc', 'Date', 'Subject'], dropna=False).size().reset_index().rename(columns={0:'count'})\n",
    "print(df_extracted.shape)\n",
    "# But regardless of the fact above, all message IDs are unique\n",
    "print(df['Message-ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then apply some basic type casting and conversions for the selected fields\n",
    "# so that they can be more thoroughly compared and profiled\n",
    "# 2 Unify the data to lower cases so they can be properly deduplicated\n",
    "for column in df_extracted.columns:\n",
    "    # this is only needed for from, to, cc and bcc columns\n",
    "    # we don't make the Subject all lower case because the upper/lower cases\n",
    "    # carry actual information, and we don't want to lose that\n",
    "    if column in ['From', 'To', 'Cc', 'Bcc']:\n",
    "        df_extracted[column] = df_extracted[column].str.lower() \n",
    "# 3 Convert the Date column from datetime.datetime to np.datetime64 so it can be properly deduplicated\n",
    "df_extracted['Date'] = pd.to_datetime(df_extracted['Date'], utc=True)\n",
    "print(df_extracted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the extracted columns types and values\n",
    "print(df_extracted.dtypes)\n",
    "print(df_extracted.describe(include='all'))\n",
    "print(df_extracted.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above we understand that all emails have a From value, and a date, but they do not always \n",
    "# have a To, Cc, Bcc, or Subject value\n",
    "# Let's have a look at the actual value distributions and validate the values\n",
    "email_regex_pattern = r\"^[a-z0-9.'_%+-<]+@[a-z0-9.-]+\\.[a-z]{2,}>*?$\"\n",
    "# From : should be valid emails\n",
    "print(df_extracted['From'].value_counts())\n",
    "print(df_extracted['From'].str.contains(email_regex_pattern).sum())\n",
    "# Only less than 10 invalid emails are in From"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date : should be within the viable operating years of Enron (~1998-2002)\n",
    "print(df_extracted['Date'].max())\n",
    "print(df_extracted['Date'].min())\n",
    "df_extracted_by_year = df_extracted.copy()\n",
    "df_extracted_by_year['year'] = df_extracted_by_year['Date'].dt.year\n",
    "print(df_extracted_by_year.groupby(['year'], dropna=False).size().reset_index().rename(columns={0:'count'}))\n",
    "# There are some emails (~400) with dates outside of the viable operating years of Enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To, Cc and Bcc : these should all be emails\n",
    "# Since these fields can be comma-delimited email addresses\n",
    "# In order to check all the emails against our regex rule, we need to explode the lists to rows first\n",
    "df_extracted_email_to = df_extracted.melt(id_vars=['From'], value_vars=['To', 'Cc', 'Bcc'])\n",
    "df_extracted_email_to = df_extracted_email_to.rename(columns={'variable': 'message_type', 'value': 'message_to'})\n",
    "df_extracted_email_to['message_to'] = df_extracted_email_to['message_to'].apply(lambda x: x.split(',') if type(x) == str else x)\n",
    "df_extracted_email_to = df_extracted_email_to[~df_extracted_email_to['message_to'].isna()].copy().reset_index(drop=True)\n",
    "df_extracted_email_to = df_extracted_email_to.explode('message_to').reset_index(drop=True)\n",
    "print(df_extracted_email_to.isna().sum())\n",
    "print(df_extracted_email_to.describe())\n",
    "print(df_extracted_email_to['message_to'].str.contains(email_regex_pattern).sum())\n",
    "# From the result we can see that only a small number of invalid email addresses exist in to To, Cc and Bcc lists\n",
    "# (~1800 invalid instances out of 1.6M instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also look at the domains of these emails (in From, To, Cc and Bcc)\n",
    "domain_regex_pattern = r'.+(@.+)$'\n",
    "df_extracted_email_to['From_domain'] = df_extracted_email_to['From'].str.extract(domain_regex_pattern)\n",
    "df_extracted_email_to['message_to_domain'] = df_extracted_email_to['message_to'].str.extract(domain_regex_pattern)\n",
    "print(df_extracted_email_to['From_domain'].value_counts())\n",
    "print(df_extracted_email_to['message_to_domain'].value_counts())\n",
    "# From the result we can see that the majority of the emails are sent between email addresses\n",
    "# in the enron.com domain. Also due to the difference in how the email is stored, some of\n",
    "# the enron email addresses are of the format e.g. \"legal <.smith@enron.com>\".\n",
    "# a further specific check on this format reveals that the email addresses within <> do not exist\n",
    "# as in the regular name@domain format, so it is safe to keep them and use them as they are to identify\n",
    "# unique senders/receivers\n",
    "check_domain_pattern = r'^.+<.+@.+\\.[a-z]{2,}>$'\n",
    "print((df_extracted_email_to[df_extracted_email_to['From'].str.contains(check_domain_pattern)]['From']).shape)\n",
    "print((df_extracted_email_to[df_extracted_email_to['From'].str.contains(check_domain_pattern)]['From']).value_counts())\n",
    "print((df_extracted_email_to[df_extracted_email_to['message_to'].str.contains(check_domain_pattern)]['message_to']).shape)\n",
    "print((df_extracted_email_to[df_extracted_email_to['message_to'].str.contains(check_domain_pattern)]['message_to']).value_counts())\n",
    "# Subject : anything can be used as a subject so no validation is needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
